<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Setting up your own Ollama LLM agent | Nathan T. Bartley </title> <meta name="author" content="Nathan T. Bartley"> <meta name="description" content="Set up your own local LLM"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="canonical" href="https://bartleyn.github.io/blog/2026/setting-up-remote-chat-ollama/"> <script>
  if (location.hostname === "www.bartleyn.github.io") {
    location.replace("https://bartleyn.github.io" + location.pathname);
  }
</script> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bartleyn.github.io/blog/2026/setting-up-remote-chat-ollama/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Nathan</span> T. Bartley </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Setting up your own Ollama LLM agent</h1> <p class="post-meta"> Created on January 04, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/agent"> <i class="fa-solid fa-hashtag fa-sm"></i> agent</a>   ·   <a href="/blog/category/projects"> <i class="fa-solid fa-tag fa-sm"></i> projects</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I’ve really come around on making use of AI (i.e., LLMs) as a tool for quickly iterating on ideas and making the rote work of software development and analysis smoother. I’ve also realized that I don’t really want to continue incentivizing the construction of large datacenters and the ravenous appetite for more and more computing resources. In my experience both in academia and my limited experience in industry, I realized that you don’t really need the biggest models to get what you need done. Don’t get me wrong, the biggest models do work really well, but if we’re faced with the true cost of a token, at what point is it worth it to roll your own solution? I anticipate we’ll collectively be faced with this sometime in the next couple of years, and even if we aren’t it’s worth evaluating the marginal utility of the big models over the small ones especially for narrow-defined tasks.</p> <p>To this end I wanted to play with ‘small’ LLMs: I have a Mac Mini with 16GB of RAM, so I set out to see if I can run my own LLM and use it to serve my development/assistant needs. I decided to set up my computer as a local server that will run Ollama and a series of connected servers to act as my own ‘local’ LLM that I can use as a drop-in replacement in my workflows (i.e., consulting with an LLM and as a code assistant in Cursor/VS Code). I’ll accept a little latency if it means I know where all the compute is happening and how my data is being used.</p> <p>In this post I’ll describe how to set up Ollama and a simple chat interface that connects to whatever LLM you want that you can fit on your machine. I arbitrarily decided on the latest Gemma3 4b model that was quantized in such a way that it can run on my Mac Mini M4 with 16GB of RAM, with plenty of room to spare for enabling different behaviors of the model. I’ve since given the local model the ability to use tools like web-search, and I’ll supply the code I used to build this. It should be straightforward to take this and run it on anywhere with a docker container.</p> <p>Here’s a rough diagram of how you’ll interact with the LLM: your browser -&gt; Gradio chat interface -&gt; chat gateway | =&gt; a) tool server | 1) SearXNG search engine |===&gt; b) LLM hosted via ollama API</p> <p>Here’s what you’ll eventually be installing (if you don’t have it already): 1) Docker Desktop 2) Ollama 3) uv / python 4) Gradio 5) Tailscale (recommended for remote access)</p> <h2 id="installing-ollama-and-downloading-llms">Installing Ollama and downloading LLMs</h2> <p>Ollama is an open source tool for running large language models (LLMs) locally on your own machine. It was built originally upon the AI inference engine llama.cpp, which is the key library that allows running inference on pre-trained LLMs. They have since expanded and stream-lined their tooling to enable the use of multimodal models and cloud-provider models. Their UI is extremely smooth, however we will largely be interacting with it through the CLI and the API it will run for us to talk to our models.</p> <p>The first step will be to download Ollama at (https://ollama.com/download)[https://ollama.com/download] on the machine you want to use as a server (here my Mac Mini I’ll call <code class="language-plaintext highlighter-rouge">mm</code>). In a terminal on <code class="language-plaintext highlighter-rouge">mm</code> issue the following to make sure it’s installed correctly:</p> <p><code class="language-plaintext highlighter-rouge">ollama --version</code></p> <p>You should see a line that reads: <code class="language-plaintext highlighter-rouge">ollama version is x.xx.x</code></p> <p>Take a look at the rest of the interface if you’re not familiar with it. If you issue a simple <code class="language-plaintext highlighter-rouge">ollama</code> command you should see the list of commands and flags you can run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Usage:
  ollama <span class="o">[</span>flags]
  ollama <span class="o">[</span><span class="nb">command</span><span class="o">]</span>

Available Commands:
  serve       Start ollama
  create      Create a model
  show        Show information <span class="k">for </span>a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  signin      Sign <span class="k">in </span>to ollama.com
  signout     Sign out from ollama.com
  list        List models
  ps          List running models
  <span class="nb">cp          </span>Copy a model
  <span class="nb">rm          </span>Remove a model
  <span class="nb">help        </span>Help about any <span class="nb">command

</span>Flags:
  <span class="nt">-h</span>, <span class="nt">--help</span>      <span class="nb">help </span><span class="k">for </span>ollama
  <span class="nt">-v</span>, <span class="nt">--version</span>   Show version information
</code></pre></div></div> <p>Here we will be largely concerning ourselves with <code class="language-plaintext highlighter-rouge">serve</code> <code class="language-plaintext highlighter-rouge">pull</code> <code class="language-plaintext highlighter-rouge">run</code> <code class="language-plaintext highlighter-rouge">stop</code> <code class="language-plaintext highlighter-rouge">list</code> <code class="language-plaintext highlighter-rouge">ps</code></p> <p>To download a model from the Ollama model library you issue a pull command with a <code class="language-plaintext highlighter-rouge">&lt;Model&gt;:&lt;tag&gt;</code> parameter specifying which model and type you want:</p> <p><code class="language-plaintext highlighter-rouge">ollama pull gemma3:4b</code></p> <p>This will pull the Gemma3 model with 4 billion parameters. Other Gemma3 models available are 1B, 4B, 12B, and 27B parameters, each of which increases the amount of RAM necessary to run the model. It might sound like a lot, but these models are largely quantized, so they don’t actually occupy a lot of RAM (though it does blow up pretty fast as you travel up in # parameters)</p> <p>Issue the following command to interact with your Gemma3 model to test that it works</p> <p><code class="language-plaintext highlighter-rouge">ollama run gemma3:4b</code></p> <p>This will open a chat interface on the command line you can issue commands with.</p> <p>Now that we’ve downloaded Ollama and a model we’ll move on to actually building the tooling around it we’ll give the model tools to use.</p> <h2 id="tailscale-for-remote-connection">Tailscale for Remote Connection</h2> <p>The second component to this, if you want to reliably make use of your secondary machine, is to install some sort of VPN that will connect the machine running the LLM and your local machine. It is more secure than just forwarding and exposing your LLM machine to the open internet. To this end I suggest installing something like Tailscale, which you will install and run on both your LLM machine and on the machine you want to access the LLM on.</p> <p>Note: I use Tailscale here because it ultimately required no setup to enable my two machines to talk to each other. Other solutions would have required installing something like OpenVPN and/or changing out my router. <strong>This setup is designed for private use over a VPN. You should not expose it publicly without authentication and rate limits</strong></p> <p>When you turn Tailscale on, you should be able to find in the tailscale settings an IP address for both machines (the usual range is 100.<em>.</em>.*). Use this to issue an <code class="language-plaintext highlighter-rouge">ssh</code> command to see if you can successfully login remotely to your LLM machine.</p> <h2 id="searxng-for-searching-with-nginx">SearXNG for Searching (With nginx)</h2> <p>Now that we can access the LLM machine remotely, we can start working towards a tool server that affords the LLM to issue tool commands, allowing it to interact with components that exist outside of the chat session. Here we will stand up a SearXNG instance, which is an open-source metasearch engine that can search across numerous search services (spanning ~200 services from Google to Wikidata to OpenAlex to Public Domain Image Archive).</p> <h3 id="docker">Docker</h3> <p>Docker saves us a lot of trouble in setting up this instance – we can just use the <code class="language-plaintext highlighter-rouge">docker compose</code> command with a docker-compose.yaml file that points to an official searxng docker image. Here’s an example docker-compose.yaml</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">services</span><span class="pi">:</span>
  <span class="na">searxng</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">searxng/searxng:latest</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">searxng</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">INSTANCE_NAME=local-searxng</span>
      <span class="pi">-</span> <span class="s">BASE_URL=http://localhost:8080/</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">searxng-data:/etc/searxng</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">unless-stopped</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">searxnet</span>

  <span class="na">nginx</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:alpine</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">searxng-nginx</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">127.0.0.1:8080:8080"</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./nginx.conf:/etc/nginx/nginx.conf:ro</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">searxng</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">unless-stopped</span>
    <span class="na">networks</span><span class="pi">:</span>
</code></pre></div></div> <p>Make sure that your docker daemon is running, and then issuing a simple <code class="language-plaintext highlighter-rouge">docker compose up -d</code> in the same directory will look for your docker-compose.yaml file and start up the nginx &amp; searxng services. For context, the nginx is included to act as a reverse proxy, so the searxng service isn’t caught off-guard by unexpected traffic requests.</p> <h4 id="updating-the-settings">Updating the Settings</h4> <p>In the settings.yaml file in the searxng container, find the following and make sure to add <code class="language-plaintext highlighter-rouge">json</code>:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">search</span><span class="pi">:</span>
  <span class="na">formats</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">html</span>
    <span class="pi">-</span> <span class="s">json</span>
</code></pre></div></div> <p>And be sure to allow unthrottled requests:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">server</span><span class="pi">:</span>
  <span class="na">limiter</span><span class="pi">:</span> <span class="kc">false</span>
</code></pre></div></div> <p>When you change those notes, you would need to spin down and up your container:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose down
docker compose up -d
</code></pre></div></div> <h2 id="tool-server">Tool Server</h2> <p>If you have gotten to this point, you should be able to talk with your Ollama model separately, and you should have a docker container running for your nginx/searXNG setup. Now we need to start to hook things up so that the model you’ll interact with can actually issue web-searches. Here’s an example of what the tool server can look like:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">Python
</span><span class="sb">from fastapi import FastAPI
from pydantic import BaseModel
import httpx
from bs4 import BeautifulSoup
from readability import Document
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
)
logger = logging.getLogger('toolServer')
logger.setLevel(logging.DEBUG)

app = FastAPI(title="Local Tool Server")

TOOLURL = "http://localhost:8080/search"

class SearchReq(BaseModel):
     query: str
     num_results: int=5

class OpenReq(BaseModel):
     url: str
     max_chars: int = 200000

# Enables the search tool for an agent
@app.post("/search")
async def search(req: SearchReq):
   params = {"q": req.query, "format":"json"}
   logger.debug("Params for search: %s", params)
   async with httpx.AsyncClient(timeout=20) as client:
       r = await client.get(TOOLURL, params=params)
       r.raise_for_status()
       data = r.json()
   logger.debug("Returned data from search: %s", data)
   results = []
   if data.get("results") is not None:
       for item in data.get("results")[:req.num_results]:
            results.append({
               "title": item.get("title"),
               "url": item.get("url"),
               "snippet": item.get("content"),
               "source": item.get("engine"),
            })
   print(results)
   return {"results": results}

# Enables opening a website for crawling for an agent
@app.post("/open")
async def open_url(req: OpenReq):
   async with httpx.AsyncClient(timeout=25, follow_redirects=True) as client:
      r = await client.get(req.url)
      r.raise_for_status()
      html = r.text
   
   doc = Document(html)
   title = doc.short_title()
   cleaned_html = doc.summary()
   
   soup = BeautifulSoup(cleaned_html, "html.parser")
   text = soup.get_text("\n", stripe=True)

   if len(text) &gt; req.max_chars:
       text = text[:req.max_chars] + "\n..."
   return {"title": title, "url": req.url, "text":text}</span>
<span class="p">```</span>
</code></pre></div></div> <p>You can run this with a command like:</p> <p><code class="language-plaintext highlighter-rouge">uvicorn tool_server:app --host [tailscale IP] --port [tool_port]</code></p> <h2 id="chat-gateway">Chat Gateway</h2> <p>Now we need a means to connect the model with the tool server, and then connect the model with the user. Here’s an example uvicorn script we can run for this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">Python
</span><span class="sb">
import json
import re
import requests
from fastapi import FastAPI
from pydantic import BaseModel
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
)

logger = logging.getLogger("chat_gateway")
logger.setLevel(logging.DEBUG)



OLLAMA_URL = "http://127.0.0.1:11434/v1/chat/completions"
TOOL_SERVER_URL = "http://127.0.0.1:[tool_port]" 
MODEL = "gemma3:latest"

def strip_code_fences(text: str) -&gt; str:
    t = text.strip()

    if t.startswith("```"):
        # remove opening ``` or ```json
        t = re.sub(r"^\s*```[a-zA-Z0-9_-]*\s*", "", t)
        # remove closing ```
        t = re.sub(r"\s*```\s*$", "", t)

    return t.strip()

SYSTEM = """You are a tool-using assistant. Follow these rules exactly.

TOOLS
- search: {"query": string, "top_k": integer}

WHEN TO CALL TOOLS
- If the user asks about recent info, news, current events, or anything you are not certain about, you MUST call search.
- If the user asks for sources/citations, you MUST call search unless the user provided the sources in the conversation.

TOOL CALL FORMAT (STRICT)
When calling a tool, output ONLY ONE JSON object and nothing else. And if you include a tool call, you must only include the tool call, no extra text before or after it. No narration.
No markdown. No code fences. No extra text.
Schema:
{"tool":"search","args":{"query":"...","top_k":5}}

AFTER TOOL RESULTS (STRICT GROUNDING)
- Use ONLY the tool results to answer. Do NOT use outside knowledge.
- Every factual claim must be supported by at least one URL from the tool results.
- If the tool results do not contain the answer, say: "I couldn't find this in the search results."

CITATIONS
- Cite sources inline using parentheses with the URL, e.g. (https://example.com).
- Do not invent URLs.

OUTPUT
- If you are calling a tool: output the tool-call JSON only. No text before or after. No narration.
- Otherwise: output a normal answer with citations when required.
"""


app = FastAPI()

class ChatIn(BaseModel):
    message: str
    history: list[dict] = [] 

#for checking if there's a tool call
TOOL_JSON_RE = re.compile(r"^\s*\{.*\}\s*$", re.S)

def ollama_chat(messages):
    r = requests.post(OLLAMA_URL, json={"model": MODEL, "messages": messages}, timeout=300)
    return r.json()["choices"][0]["message"]["content"]

def call_tool(tool, args):
    if tool == "search":
        logger.debug("Calling search tool with args: %s", args) 
        # tool call should be a POST to  /search with {"query":..., "top_k":...}
        r = requests.post(f"{TOOL_SERVER_URL}/search", json=args, timeout=60)
        r.raise_for_status()
        logger.debug("Reply from tool: %s", r.json())
        return r.json()
    raise ValueError(f"Unknown tool: {tool}")

@app.post("/chat")
def chat(payload: ChatIn):
    logger.info("Received chat request")
    messages = [{"role": "system", "content": SYSTEM}] + payload.history + [{"role": "user", "content": payload.message}]
    logger.debug("Payload: %s", payload)

    assistant = ollama_chat(messages)
    logger.debug("First reply from initial chat: %s", assistant)
    # If the model requested a tool call:
    if TOOL_JSON_RE.match(strip_code_fences(assistant.strip())):
        call = json.loads(strip_code_fences(assistant.strip()))
        logger.debug("Tool found")
        tool_result = call_tool(call["tool"], call.get("args", {}))

        # Feed the tool result back
        messages.append({"role": "assistant", "content": assistant})
        messages.append({
          "role": "user",
          "content": (
          "Tool result (JSON):\n"
          f"{json.dumps(tool_result, ensure_ascii=False)}\n\n"
          "Now answer the user's question using ONLY the tool result. "
          "If you cite sources, include the URL in parentheses."
          )
        })
        logger.debug("Messages: %s", messages)
        assistant = ollama_chat(messages)
    logger.debug("Raw reply: %s", assistant)
    return {"reply": assistant}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Run the chat gateway much like the toolserver:</p> <p><code class="language-plaintext highlighter-rouge">uvicorn chat_gateway:app --host [tailscale IP] --port [chat_gateway_port]</code></p> <p>Now you should have the ability to POST a request to your chat gateway that will respond to a message you send it. For instance</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">bash
</span>curl <span class="nt">-s</span> http://[remote_machine_tailscale_ip]:[chat_gateway_port]/chat <span class="se">\ </span>                                                                                 
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{"message":"Hello! Just say hi back.","history":[]}'</span> <span class="se">\</span>
  <span class="p">;</span> <span class="nb">echo</span>

<span class="o">{</span><span class="s2">"reply"</span>:<span class="s2">"Hi back! How can I help you today?"</span><span class="o">}</span>
<span class="p">```</span>
</code></pre></div></div> <p>You can also confirm that it can web-search with a specific query that is past it’s training date:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">bash
</span>
curl <span class="nt">-s</span> http://[remote_machine_tailscale_ip]:[chat_gateway_port]/chat <span class="se">\ </span>                                                   
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{"message":"Hello! Can you search for news results about Venezuela and date it?.","history":[]}'</span> <span class="se">\</span>
  <span class="p">;</span> <span class="nb">echo</span>

<span class="o">{</span><span class="s2">"reply"</span>:<span class="s2">"Here’s a summary of recent news regarding Venezuela, as of the search results:</span><span class="se">\n\n</span><span class="s2">As of today, January 9, 2026, the United States has seized its fifth oil tanker linked to Venezuela (https://abcnews.go.com/International/live-updates/venezuela-live-updates-trump-give-details-after-us/?id=127792811). U.S. President Donald Trump met with oil company executives to discuss Venezuela (https://www.reuters.com/world/venezuela/). The U.S. has long demanded the release of detained Venezuelan President Nicolás Maduro (https://www.bbc.com/news/topics/cg41ylwvwgxt).  Venezuelaanalysis provides ongoing news and analysis (https://venezuelanalysis.com/)."</span><span class="o">}</span>
<span class="p">```</span>
<span class="p">````</span><span class="nl">markdown
</span>

<span class="gu">## Chat Gradio UI</span>

If that wasn't enough for you, and you want some sort of UI to interact with the model, we'll run through a simple Gradio UI that can start as a means for replicating your own ChatGPT-like interface

<span class="p">````</span>markdown
<span class="p">```</span><span class="nl">Python
</span><span class="sb">import requests
import gradio as gr

CHAT_GATEWAY_URL = 'http://[remote_machine_tailscale_ip]:[chat_gateway_port]/chat'


def to_gateway_history(chat_history):
  # gr.ChatInterface expects [(user, assistant),...]
  out = []
  if chat_history is None:
    return out
  for u, a in chat_history:
    if u:
      out.append({"role": "user", "content": u})
    if a:
      out.append({"role": "assistant", "content": a})
  return out
 
def one_turn(user_message: str) -&gt; str:
  r = requests.post(
          CHAT_GATEWAY_URL,
          json={'message': user_message, 'history': []},
          timeout=120
          )
  r.raise_for_status()
  return r.json()['reply']

demo = gr.Interface(
         fn=one_turn,
         inputs=gr.Textbox(label="User"),
         outputs=gr.Textbox(label="Response"),
         title="Chat Gateway UI"
       )

if __name__ == '__main__':
  demo.launch(server_name="[remote_machine_tailscale_interface]", server_port=[ui_port])</span>
<span class="p">```</span>
</code></pre></div></div> <p>This should give you the a rough interface that you can log into on your local machine’s browser (http://xyz:port)!</p> <p>Code for the current version of this project will be found at TODO: XYZ</p> <h2 id="conclusion">Conclusion</h2> <p>Here’s what you should have in startup order:</p> <ol> <li><code class="language-plaintext highlighter-rouge">ollama serve</code></li> <li><code class="language-plaintext highlighter-rouge">docker compose up -d (searxng)</code></li> <li><code class="language-plaintext highlighter-rouge">uvicorn tool_server</code></li> <li><code class="language-plaintext highlighter-rouge">uvicorn chat_gateway</code></li> <li><code class="language-plaintext highlighter-rouge">python gradio_ui.py</code></li> </ol> <p>As a development note, I run my local servers all on their own <code class="language-plaintext highlighter-rouge">screen</code>, perhaps you use <code class="language-plaintext highlighter-rouge">tmux</code> but you should consider a tool like this to quickly manage the locally deployed servers. There are also likely better ways to actually run these servers, but that’s out of the scope of this post.</p> <p>Thanks for the time, happy building.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/recovering-mysql-data-from-myd-myi-frm/">Recovering MySQL data from .MYI, .MYD, .FRM files</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/getting-vscode-cpp-working-on-macos/">Getting VSCode C++ to work on MacOS</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Nathan T. Bartley. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>