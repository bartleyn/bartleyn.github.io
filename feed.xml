<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://bartleyn.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bartleyn.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-15T23:04:13+00:00</updated><id>https://bartleyn.github.io/feed.xml</id><title type="html">blank</title><subtitle>Academic &amp; Professional Website for Nathan Bartley </subtitle><entry><title type="html">Setting up your own Ollama LLM agent</title><link href="https://bartleyn.github.io/blog/2026/setting-up-remote-chat-ollama/" rel="alternate" type="text/html" title="Setting up your own Ollama LLM agent"/><published>2026-01-04T17:17:16+00:00</published><updated>2026-01-04T17:17:16+00:00</updated><id>https://bartleyn.github.io/blog/2026/setting-up-remote-chat-ollama</id><content type="html" xml:base="https://bartleyn.github.io/blog/2026/setting-up-remote-chat-ollama/"><![CDATA[<p>I’ve really come around on making use of AI (i.e., LLMs) as a tool for quickly iterating on ideas and making the rote work of software development and analysis smoother. I’ve also realized that I don’t really want to continue incentivizing the construction of large datacenters and the ravenous appetite for more and more computing resources. In my experience both in academia and my limited experience in industry, I realized that you don’t really need the biggest models to get what you need done. Don’t get me wrong, the biggest models do work really well, but if we’re faced with the true cost of a token, at what point is it worth it to roll your own solution? I anticipate we’ll collectively be faced with this sometime in the next couple of years, and even if we aren’t it’s worth evaluating the marginal utility of the big models over the small ones especially for narrow-defined tasks.</p> <p>To this end I wanted to play with ‘small’ LLMs: I have a Mac Mini with 16GB of RAM, so I set out to see if I can run my own LLM and use it to serve my development/assistant needs. I decided to set up my computer as a local server that will run Ollama and a series of connected servers to act as my own ‘local’ LLM that I can use as a drop-in replacement in my workflows (i.e., consulting with an LLM and as a code assistant in Cursor/VS Code). I’ll accept a little latency if it means I know where all the compute is happening and how my data is being used.</p> <p>In this post I’ll describe how to set up Ollama and a simple chat interface that connects to whatever LLM you want that you can fit on your machine. I arbitrarily decided on the latest Gemma3 4b model that was quantized in such a way that it can run on my Mac Mini M4 with 16GB of RAM, with plenty of room to spare for enabling different behaviors of the model. I’ve since given the local model the ability to use tools like web-search, and I’ll supply the code I used to build this. It should be straightforward to take this and run it on anywhere with a docker container.</p> <p>Here’s a rough diagram of how you’ll interact with the LLM: your browser -&gt; Gradio chat interface -&gt; chat gateway | =&gt; a) tool server | 1) SearXNG search engine |===&gt; b) LLM hosted via ollama API</p> <p>Here’s what you’ll eventually be installing (if you don’t have it already): 1) Docker Desktop 2) Ollama 3) uv / python 4) Gradio 5) Tailscale (recommended for remote access)</p> <h2 id="installing-ollama-and-downloading-llms">Installing Ollama and downloading LLMs</h2> <p>Ollama is an open source tool for running large language models (LLMs) locally on your own machine. It was built originally upon the AI inference engine llama.cpp, which is the key library that allows running inference on pre-trained LLMs. They have since expanded and stream-lined their tooling to enable the use of multimodal models and cloud-provider models. Their UI is extremely smooth, however we will largely be interacting with it through the CLI and the API it will run for us to talk to our models.</p> <p>The first step will be to download Ollama at (https://ollama.com/download)[https://ollama.com/download] on the machine you want to use as a server (here my Mac Mini I’ll call <code class="language-plaintext highlighter-rouge">mm</code>). In a terminal on <code class="language-plaintext highlighter-rouge">mm</code> issue the following to make sure it’s installed correctly:</p> <p><code class="language-plaintext highlighter-rouge">ollama --version</code></p> <p>You should see a line that reads: <code class="language-plaintext highlighter-rouge">ollama version is x.xx.x</code></p> <p>Take a look at the rest of the interface if you’re not familiar with it. If you issue a simple <code class="language-plaintext highlighter-rouge">ollama</code> command you should see the list of commands and flags you can run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Usage:
  ollama <span class="o">[</span>flags]
  ollama <span class="o">[</span><span class="nb">command</span><span class="o">]</span>

Available Commands:
  serve       Start ollama
  create      Create a model
  show        Show information <span class="k">for </span>a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  signin      Sign <span class="k">in </span>to ollama.com
  signout     Sign out from ollama.com
  list        List models
  ps          List running models
  <span class="nb">cp          </span>Copy a model
  <span class="nb">rm          </span>Remove a model
  <span class="nb">help        </span>Help about any <span class="nb">command

</span>Flags:
  <span class="nt">-h</span>, <span class="nt">--help</span>      <span class="nb">help </span><span class="k">for </span>ollama
  <span class="nt">-v</span>, <span class="nt">--version</span>   Show version information
</code></pre></div></div> <p>Here we will be largely concerning ourselves with <code class="language-plaintext highlighter-rouge">serve</code> <code class="language-plaintext highlighter-rouge">pull</code> <code class="language-plaintext highlighter-rouge">run</code> <code class="language-plaintext highlighter-rouge">stop</code> <code class="language-plaintext highlighter-rouge">list</code> <code class="language-plaintext highlighter-rouge">ps</code></p> <p>To download a model from the Ollama model library you issue a pull command with a <code class="language-plaintext highlighter-rouge">&lt;Model&gt;:&lt;tag&gt;</code> parameter specifying which model and type you want:</p> <p><code class="language-plaintext highlighter-rouge">ollama pull gemma3:4b</code></p> <p>This will pull the Gemma3 model with 4 billion parameters. Other Gemma3 models available are 1B, 4B, 12B, and 27B parameters, each of which increases the amount of RAM necessary to run the model. It might sound like a lot, but these models are largely quantized, so they don’t actually occupy a lot of RAM (though it does blow up pretty fast as you travel up in # parameters)</p> <p>Issue the following command to interact with your Gemma3 model to test that it works</p> <p><code class="language-plaintext highlighter-rouge">ollama run gemma3:4b</code></p> <p>This will open a chat interface on the command line you can issue commands with.</p> <p>Now that we’ve downloaded Ollama and a model we’ll move on to actually building the tooling around it we’ll give the model tools to use.</p> <h2 id="tailscale-for-remote-connection">Tailscale for Remote Connection</h2> <p>The second component to this, if you want to reliably make use of your secondary machine, is to install some sort of VPN that will connect the machine running the LLM and your local machine. It is more secure than just forwarding and exposing your LLM machine to the open internet. To this end I suggest installing something like Tailscale, which you will install and run on both your LLM machine and on the machine you want to access the LLM on.</p> <p>Note: I use Tailscale here because it ultimately required no setup to enable my two machines to talk to each other. Other solutions would have required installing something like OpenVPN and/or changing out my router. <strong>This setup is designed for private use over a VPN. You should not expose it publicly without authentication and rate limits</strong></p> <p>When you turn Tailscale on, you should be able to find in the tailscale settings an IP address for both machines (the usual range is 100.<em>.</em>.*). Use this to issue an <code class="language-plaintext highlighter-rouge">ssh</code> command to see if you can successfully login remotely to your LLM machine.</p> <h2 id="searxng-for-searching-with-nginx">SearXNG for Searching (With nginx)</h2> <p>Now that we can access the LLM machine remotely, we can start working towards a tool server that affords the LLM to issue tool commands, allowing it to interact with components that exist outside of the chat session. Here we will stand up a SearXNG instance, which is an open-source metasearch engine that can search across numerous search services (spanning ~200 services from Google to Wikidata to OpenAlex to Public Domain Image Archive).</p> <h3 id="docker">Docker</h3> <p>Docker saves us a lot of trouble in setting up this instance – we can just use the <code class="language-plaintext highlighter-rouge">docker compose</code> command with a docker-compose.yaml file that points to an official searxng docker image. Here’s an example docker-compose.yaml</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">services</span><span class="pi">:</span>
  <span class="na">searxng</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">searxng/searxng:latest</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">searxng</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">INSTANCE_NAME=local-searxng</span>
      <span class="pi">-</span> <span class="s">BASE_URL=http://localhost:8080/</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">searxng-data:/etc/searxng</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">unless-stopped</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">searxnet</span>

  <span class="na">nginx</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:alpine</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">searxng-nginx</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">127.0.0.1:8080:8080"</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./nginx.conf:/etc/nginx/nginx.conf:ro</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">searxng</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">unless-stopped</span>
    <span class="na">networks</span><span class="pi">:</span>
</code></pre></div></div> <p>Make sure that your docker daemon is running, and then issuing a simple <code class="language-plaintext highlighter-rouge">docker compose up -d</code> in the same directory will look for your docker-compose.yaml file and start up the nginx &amp; searxng services. For context, the nginx is included to act as a reverse proxy, so the searxng service isn’t caught off-guard by unexpected traffic requests.</p> <h4 id="updating-the-settings">Updating the Settings</h4> <p>In the settings.yaml file in the searxng container, find the following and make sure to add <code class="language-plaintext highlighter-rouge">json</code>:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">search</span><span class="pi">:</span>
  <span class="na">formats</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">html</span>
    <span class="pi">-</span> <span class="s">json</span>
</code></pre></div></div> <p>And be sure to allow unthrottled requests:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">server</span><span class="pi">:</span>
  <span class="na">limiter</span><span class="pi">:</span> <span class="kc">false</span>
</code></pre></div></div> <p>When you change those notes, you would need to spin down and up your container:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose down
docker compose up -d
</code></pre></div></div> <h2 id="tool-server">Tool Server</h2> <p>If you have gotten to this point, you should be able to talk with your Ollama model separately, and you should have a docker container running for your nginx/searXNG setup. Now we need to start to hook things up so that the model you’ll interact with can actually issue web-searches. Here’s an example of what the tool server can look like:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">Python
</span><span class="sb">from fastapi import FastAPI
from pydantic import BaseModel
import httpx
from bs4 import BeautifulSoup
from readability import Document
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
)
logger = logging.getLogger('toolServer')
logger.setLevel(logging.DEBUG)

app = FastAPI(title="Local Tool Server")

TOOLURL = "http://localhost:8080/search"

class SearchReq(BaseModel):
     query: str
     num_results: int=5

class OpenReq(BaseModel):
     url: str
     max_chars: int = 200000

# Enables the search tool for an agent
@app.post("/search")
async def search(req: SearchReq):
   params = {"q": req.query, "format":"json"}
   logger.debug("Params for search: %s", params)
   async with httpx.AsyncClient(timeout=20) as client:
       r = await client.get(TOOLURL, params=params)
       r.raise_for_status()
       data = r.json()
   logger.debug("Returned data from search: %s", data)
   results = []
   if data.get("results") is not None:
       for item in data.get("results")[:req.num_results]:
            results.append({
               "title": item.get("title"),
               "url": item.get("url"),
               "snippet": item.get("content"),
               "source": item.get("engine"),
            })
   print(results)
   return {"results": results}

# Enables opening a website for crawling for an agent
@app.post("/open")
async def open_url(req: OpenReq):
   async with httpx.AsyncClient(timeout=25, follow_redirects=True) as client:
      r = await client.get(req.url)
      r.raise_for_status()
      html = r.text
   
   doc = Document(html)
   title = doc.short_title()
   cleaned_html = doc.summary()
   
   soup = BeautifulSoup(cleaned_html, "html.parser")
   text = soup.get_text("\n", stripe=True)

   if len(text) &gt; req.max_chars:
       text = text[:req.max_chars] + "\n..."
   return {"title": title, "url": req.url, "text":text}</span>
<span class="p">```</span>
</code></pre></div></div> <p>You can run this with a command like:</p> <p><code class="language-plaintext highlighter-rouge">uvicorn tool_server:app --host [tailscale IP] --port [tool_port]</code></p> <h2 id="chat-gateway">Chat Gateway</h2> <p>Now we need a means to connect the model with the tool server, and then connect the model with the user. Here’s an example uvicorn script we can run for this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">Python
</span><span class="sb">
import json
import re
import requests
from fastapi import FastAPI
from pydantic import BaseModel
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
)

logger = logging.getLogger("chat_gateway")
logger.setLevel(logging.DEBUG)



OLLAMA_URL = "http://127.0.0.1:11434/v1/chat/completions"
TOOL_SERVER_URL = "http://127.0.0.1:[tool_port]" 
MODEL = "gemma3:latest"

def strip_code_fences(text: str) -&gt; str:
    t = text.strip()

    if t.startswith("```"):
        # remove opening ``` or ```json
        t = re.sub(r"^\s*```[a-zA-Z0-9_-]*\s*", "", t)
        # remove closing ```
        t = re.sub(r"\s*```\s*$", "", t)

    return t.strip()

SYSTEM = """You are a tool-using assistant. Follow these rules exactly.

TOOLS
- search: {"query": string, "top_k": integer}

WHEN TO CALL TOOLS
- If the user asks about recent info, news, current events, or anything you are not certain about, you MUST call search.
- If the user asks for sources/citations, you MUST call search unless the user provided the sources in the conversation.

TOOL CALL FORMAT (STRICT)
When calling a tool, output ONLY ONE JSON object and nothing else. And if you include a tool call, you must only include the tool call, no extra text before or after it. No narration.
No markdown. No code fences. No extra text.
Schema:
{"tool":"search","args":{"query":"...","top_k":5}}

AFTER TOOL RESULTS (STRICT GROUNDING)
- Use ONLY the tool results to answer. Do NOT use outside knowledge.
- Every factual claim must be supported by at least one URL from the tool results.
- If the tool results do not contain the answer, say: "I couldn't find this in the search results."

CITATIONS
- Cite sources inline using parentheses with the URL, e.g. (https://example.com).
- Do not invent URLs.

OUTPUT
- If you are calling a tool: output the tool-call JSON only. No text before or after. No narration.
- Otherwise: output a normal answer with citations when required.
"""


app = FastAPI()

class ChatIn(BaseModel):
    message: str
    history: list[dict] = [] 

#for checking if there's a tool call
TOOL_JSON_RE = re.compile(r"^\s*\{.*\}\s*$", re.S)

def ollama_chat(messages):
    r = requests.post(OLLAMA_URL, json={"model": MODEL, "messages": messages}, timeout=300)
    return r.json()["choices"][0]["message"]["content"]

def call_tool(tool, args):
    if tool == "search":
        logger.debug("Calling search tool with args: %s", args) 
        # tool call should be a POST to  /search with {"query":..., "top_k":...}
        r = requests.post(f"{TOOL_SERVER_URL}/search", json=args, timeout=60)
        r.raise_for_status()
        logger.debug("Reply from tool: %s", r.json())
        return r.json()
    raise ValueError(f"Unknown tool: {tool}")

@app.post("/chat")
def chat(payload: ChatIn):
    logger.info("Received chat request")
    messages = [{"role": "system", "content": SYSTEM}] + payload.history + [{"role": "user", "content": payload.message}]
    logger.debug("Payload: %s", payload)

    assistant = ollama_chat(messages)
    logger.debug("First reply from initial chat: %s", assistant)
    # If the model requested a tool call:
    if TOOL_JSON_RE.match(strip_code_fences(assistant.strip())):
        call = json.loads(strip_code_fences(assistant.strip()))
        logger.debug("Tool found")
        tool_result = call_tool(call["tool"], call.get("args", {}))

        # Feed the tool result back
        messages.append({"role": "assistant", "content": assistant})
        messages.append({
          "role": "user",
          "content": (
          "Tool result (JSON):\n"
          f"{json.dumps(tool_result, ensure_ascii=False)}\n\n"
          "Now answer the user's question using ONLY the tool result. "
          "If you cite sources, include the URL in parentheses."
          )
        })
        logger.debug("Messages: %s", messages)
        assistant = ollama_chat(messages)
    logger.debug("Raw reply: %s", assistant)
    return {"reply": assistant}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Run the chat gateway much like the toolserver:</p> <p><code class="language-plaintext highlighter-rouge">uvicorn chat_gateway:app --host [tailscale IP] --port [chat_gateway_port]</code></p> <p>Now you should have the ability to POST a request to your chat gateway that will respond to a message you send it. For instance</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">bash
</span>curl <span class="nt">-s</span> http://[remote_machine_tailscale_ip]:[chat_gateway_port]/chat <span class="se">\ </span>                                                                                 
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{"message":"Hello! Just say hi back.","history":[]}'</span> <span class="se">\</span>
  <span class="p">;</span> <span class="nb">echo</span>

<span class="o">{</span><span class="s2">"reply"</span>:<span class="s2">"Hi back! How can I help you today?"</span><span class="o">}</span>
<span class="p">```</span>
</code></pre></div></div> <p>You can also confirm that it can web-search with a specific query that is past it’s training date:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">bash
</span>
curl <span class="nt">-s</span> http://[remote_machine_tailscale_ip]:[chat_gateway_port]/chat <span class="se">\ </span>                                                   
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{"message":"Hello! Can you search for news results about Venezuela and date it?.","history":[]}'</span> <span class="se">\</span>
  <span class="p">;</span> <span class="nb">echo</span>

<span class="o">{</span><span class="s2">"reply"</span>:<span class="s2">"Here’s a summary of recent news regarding Venezuela, as of the search results:</span><span class="se">\n\n</span><span class="s2">As of today, January 9, 2026, the United States has seized its fifth oil tanker linked to Venezuela (https://abcnews.go.com/International/live-updates/venezuela-live-updates-trump-give-details-after-us/?id=127792811). U.S. President Donald Trump met with oil company executives to discuss Venezuela (https://www.reuters.com/world/venezuela/). The U.S. has long demanded the release of detained Venezuelan President Nicolás Maduro (https://www.bbc.com/news/topics/cg41ylwvwgxt).  Venezuelaanalysis provides ongoing news and analysis (https://venezuelanalysis.com/)."</span><span class="o">}</span>
<span class="p">```</span>
<span class="p">````</span><span class="nl">markdown
</span>

<span class="gu">## Chat Gradio UI</span>

If that wasn't enough for you, and you want some sort of UI to interact with the model, we'll run through a simple Gradio UI that can start as a means for replicating your own ChatGPT-like interface

<span class="p">````</span>markdown
<span class="p">```</span><span class="nl">Python
</span><span class="sb">import requests
import gradio as gr

CHAT_GATEWAY_URL = 'http://[remote_machine_tailscale_ip]:[chat_gateway_port]/chat'


def to_gateway_history(chat_history):
  # gr.ChatInterface expects [(user, assistant),...]
  out = []
  if chat_history is None:
    return out
  for u, a in chat_history:
    if u:
      out.append({"role": "user", "content": u})
    if a:
      out.append({"role": "assistant", "content": a})
  return out
 
def one_turn(user_message: str) -&gt; str:
  r = requests.post(
          CHAT_GATEWAY_URL,
          json={'message': user_message, 'history': []},
          timeout=120
          )
  r.raise_for_status()
  return r.json()['reply']

demo = gr.Interface(
         fn=one_turn,
         inputs=gr.Textbox(label="User"),
         outputs=gr.Textbox(label="Response"),
         title="Chat Gateway UI"
       )

if __name__ == '__main__':
  demo.launch(server_name="[remote_machine_tailscale_interface]", server_port=[ui_port])</span>
<span class="p">```</span>
</code></pre></div></div> <p>This should give you the a rough interface that you can log into on your local machine’s browser (http://xyz:port)!</p> <p>Code for the current version of this project will be found at TODO: XYZ</p> <h2 id="conclusion">Conclusion</h2> <p>Here’s what you should have in startup order:</p> <ol> <li><code class="language-plaintext highlighter-rouge">ollama serve</code></li> <li><code class="language-plaintext highlighter-rouge">docker compose up -d (searxng)</code></li> <li><code class="language-plaintext highlighter-rouge">uvicorn tool_server</code></li> <li><code class="language-plaintext highlighter-rouge">uvicorn chat_gateway</code></li> <li><code class="language-plaintext highlighter-rouge">python gradio_ui.py</code></li> </ol> <p>As a development note, I run my local servers all on their own <code class="language-plaintext highlighter-rouge">screen</code>, perhaps you use <code class="language-plaintext highlighter-rouge">tmux</code> but you should consider a tool like this to quickly manage the locally deployed servers. There are also likely better ways to actually run these servers, but that’s out of the scope of this post.</p> <p>Thanks for the time, happy building.</p>]]></content><author><name></name></author><category term="projects"/><category term="llm"/><category term="agent"/><summary type="html"><![CDATA[Set up your own local LLM]]></summary></entry><entry><title type="html">Recovering MySQL data from .MYI, .MYD, .FRM files</title><link href="https://bartleyn.github.io/blog/2021/recovering-mysql-data-from-myd-myi-frm/" rel="alternate" type="text/html" title="Recovering MySQL data from .MYI, .MYD, .FRM files"/><published>2021-10-25T21:01:00+00:00</published><updated>2021-10-25T21:01:00+00:00</updated><id>https://bartleyn.github.io/blog/2021/recovering-mysql-data-from-myd-myi-frm</id><content type="html" xml:base="https://bartleyn.github.io/blog/2021/recovering-mysql-data-from-myd-myi-frm/"><![CDATA[<h2 id="recovering-mysql-data-from-myi-myd-frm-files">Recovering MySQL data from .MYI, .MYD, .FRM files</h2> <p>###Summary</p> <ul> <li>Identifying the hexdump of the version number of the .frm file</li> <li>Installing that version of MySQL https://downloads.mysql.com/archives/community/</li> <li>Starting it up</li> <li>Resetting the password</li> <li>Creating a new database</li> <li>Moving the data to that database folder</li> <li>using mysqldump to dump the data to a CSV format</li> </ul> <h2 id="what-version-of-mysql-to-install">What version of MySQL to install?</h2> <p>Drawing inspiration from the <a href="https://www.percona.com/blog/2015/07/09/obtain-mysql-version-frm-file/">percona blog</a>, as long as the version is old enough, you can make use of the <code class="language-plaintext highlighter-rouge">hexdump</code> command to identify what version of MySQL to install.</p> <p>Execute the following hexdump call to look at the first two bytes of the <code class="language-plaintext highlighter-rouge">.frm</code> file (the version number is stored at a 0x33 offset): <code class="language-plaintext highlighter-rouge">hexdump -s 0x33 -n 2 -v -d /var/lib/mysql/tweetdata/tweets.frm</code></p> <p>For our data we got the following result:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0000033   50704                                               
0000035
</code></pre></div></div> <p>Which suggests that we should install version 5.7.4 as that must have been the last version that the table was built / altered on.</p> <p>###Downloading MySQL If you need to download an older version of MySQL you can navigate to <a href="https://downloads.mysql.com/archives/community/">this archive page</a>:</p> <div class="row mt-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mysql_archive-480.webp 480w,/assets/img/mysql_archive-800.webp 800w,/assets/img/mysql_archive-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mysql_archive.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <caption class="caption"> MySQL Archive page </caption> <p>We downloaded the .tar ball for the RPM bundle (345 M at the top).</p> <p>###Installing</p> <p>After downloading the tar ball we extract and run <code class="language-plaintext highlighter-rouge">sudo yum localinstall MySQL-shared-5.7.4_m14-1.linux_glibc2.5.x86_64.rpm</code></p> <p>on each of the <code class="language-plaintext highlighter-rouge">.rpm</code> files you need (for us this was the server, client, and shared).</p> <p><a href="https://web.archive.org/web/20170810042727/https://dev.mysql.com/doc/refman/5.5/en/mysql-install-db.html">Archive link</a></p> <p>Since we are using version 5.7.4 we can call mysql_install_db to do some heavy lifting in setup:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mysql_install_db --user=mysql \
         --basedir=/opt/mysql/mysql \
         --datadir=/opt/mysql/mysql/data
</code></pre></div></div> <p>Now you can copy your .FRM, .MYD, .MYI files into your data directory!</p> <h3 id="starting-up">Starting up</h3> <p>If your installation includes <code class="language-plaintext highlighter-rouge">mysqld_safe</code> run:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mysqld_safe --user=mysql &amp;
</code></pre></div></div> <p>You may also be able to run: <code class="language-plaintext highlighter-rouge">systemctl start mysqld</code></p> <p>Access the server with the client: <code class="language-plaintext highlighter-rouge">mysql -u root -p</code></p> <p>You will have to reset your password, and if it doesn’t prompt you already you can try:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SET PASSWORD FOR 'root'@'localhost' = PASSWORD('New_Password');
</code></pre></div></div> <p>You can then create a new database which will create a database folder in the data directory we passed to <code class="language-plaintext highlighter-rouge">mysql_install_db</code>. After copying the .FRM, .MYD, .MYI files into that directory you should be able to see a table within that directory!</p> <h3 id="dumping-the-data-to-csv">Dumping the data to CSV</h3> <p>Finally, once the table is available, you can call mysqldump to dump the data to csv:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mysqldump -u [username] -p -t -T/path/to/directory [database] [tableName] --fields-terminated-by=,
</code></pre></div></div> <p>By default it may not give you column names, which wasn’t really a problem for us, but if you need column names, there are <a href="https://stackoverflow.com/questions/262924/how-to-export-dump-a-mysql-table-into-a-text-file-including-the-field-names-a">alternate ways</a> to dump the data that may work for you.</p> <p>Hope that helps! </p>]]></content><author><name></name></author><category term="debugging"/><category term="MySQL"/><category term="Hexdump"/><summary type="html"><![CDATA[Saving you some trouble getting data out of your files]]></summary></entry><entry><title type="html">Getting VSCode C++ to work on MacOS</title><link href="https://bartleyn.github.io/blog/2021/getting-vscode-cpp-working-on-macos/" rel="alternate" type="text/html" title="Getting VSCode C++ to work on MacOS"/><published>2021-10-19T21:01:00+00:00</published><updated>2021-10-19T21:01:00+00:00</updated><id>https://bartleyn.github.io/blog/2021/getting-vscode-cpp-working-on-macos</id><content type="html" xml:base="https://bartleyn.github.io/blog/2021/getting-vscode-cpp-working-on-macos/"><![CDATA[<h2 id="connecting-clangc-compiler-to-vscode-macos">Connecting Clang/C++ compiler to VSCode (macOS)</h2> <p>This is a short post meant to step through the steps needed to get Visual Studio code (macOS) to use your clang c++ compiler to build your cpp code.</p> <h3 id="making-sure-you-have-support-for-c">Making sure you have support for c++</h3> <p>You may encounter that you don’t have any formatting colors or other options for any c++ code you write, so as a first step we suggest you install the C++ extension.</p> <div class="row mt-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vscode_1a_cpp-480.webp 480w,/assets/img/vscode_1a_cpp-800.webp 800w,/assets/img/vscode_1a_cpp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vscode_1a_cpp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <caption class="caption"> Installing the C++ extension </caption> <p>You may also need to install clang if you haven’t already. You can do this by running <code class="language-plaintext highlighter-rouge">xcode-select --install</code> in your terminal.</p> <h3 id="writing-up-sample-cpp">Writing up sample .cpp</h3> <div class="row mt-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vscode_1_samepl_code-480.webp 480w,/assets/img/vscode_1_samepl_code-800.webp 800w,/assets/img/vscode_1_samepl_code-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vscode_1_samepl_code.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <caption class="caption"> Sample .cpp file open in editor </caption> <p>After with a fresh sample .cpp file in an open editor. If you want to copy and paste:</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
    <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"lorem"</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"ipsum"</span><span class="p">;</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"do re mi "</span><span class="p">;</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"fa"</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
        <span class="n">x</span><span class="o">--</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="building-the-cpp">Building the .cpp</h3> <p>Next we need to give VSCode some instructions for how to actually “build” our code. We do this by specifying commands and instructions through a <code class="language-plaintext highlighter-rouge">tasks.json</code> file. When you go to Terminal -&gt; Run Build Task you will be presented with a search bar and a result saying there is no build task. We will configure this to add a build command.</p> <div class="row mt-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vscode_2_build_task-480.webp 480w,/assets/img/vscode_2_build_task-800.webp 800w,/assets/img/vscode_2_build_task-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vscode_2_build_task.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This will open a tasks.json file that is empty. Here is my tasks.json file contents:</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="err">    // See https://go.microsoft.com/fwlink/?LinkId=</span><span class="mi">733558</span><span class="w">
    </span><span class="err">    // for the documentation about the tasks.json format</span><span class="w">
    </span><span class="err">    </span><span class="nl">"version"</span><span class="p">:</span><span class="err"> </span><span class="s2">"2.0.0"</span><span class="p">,</span><span class="w">
    </span><span class="err">    </span><span class="nl">"tasks"</span><span class="p">:</span><span class="err"> </span><span class="p">[</span><span class="w">
    </span><span class="err">      </span><span class="p">{</span><span class="w">
    </span><span class="err">        </span><span class="nl">"type"</span><span class="p">:</span><span class="err"> </span><span class="s2">"shell"</span><span class="p">,</span><span class="w">
    </span><span class="err">        </span><span class="nl">"label"</span><span class="p">:</span><span class="err"> </span><span class="s2">"clang++ build active file"</span><span class="p">,</span><span class="w">
    </span><span class="err">        </span><span class="nl">"command"</span><span class="p">:</span><span class="err"> </span><span class="s2">"/usr/bin/clang++"</span><span class="p">,</span><span class="w">
    </span><span class="err">        </span><span class="nl">"args"</span><span class="p">:</span><span class="err"> </span><span class="p">[</span><span class="w">
    </span><span class="err">          </span><span class="s2">"-std=c++17"</span><span class="p">,</span><span class="w">
    </span><span class="err">          </span><span class="s2">"-stdlib=libc++"</span><span class="p">,</span><span class="w">
    </span><span class="err">          //</span><span class="s2">"-v"</span><span class="p">,</span><span class="w">
    </span><span class="err">          </span><span class="s2">"-g"</span><span class="p">,</span><span class="w">
    </span><span class="err">          </span><span class="s2">"${file}"</span><span class="p">,</span><span class="w">
    </span><span class="err">          </span><span class="s2">"-o"</span><span class="p">,</span><span class="w">
    </span><span class="err">          </span><span class="s2">"${fileDirname}/${fileBasenameNoExtension}"</span><span class="w">
    </span><span class="err">        </span><span class="p">],</span><span class="w">
    </span><span class="err">        </span><span class="nl">"options"</span><span class="p">:</span><span class="err"> </span><span class="p">{</span><span class="w">
    </span><span class="err">          </span><span class="nl">"cwd"</span><span class="p">:</span><span class="err"> </span><span class="s2">"${workspaceFolder}"</span><span class="w">
    </span><span class="err">        </span><span class="p">},</span><span class="w">
    </span><span class="err">        </span><span class="nl">"problemMatcher"</span><span class="p">:</span><span class="err"> </span><span class="p">{</span><span class="w">
    </span><span class="err">            </span><span class="nl">"owner"</span><span class="p">:</span><span class="err"> </span><span class="s2">"cpp"</span><span class="p">,</span><span class="w">
    </span><span class="err">            </span><span class="nl">"fileLocation"</span><span class="p">:</span><span class="err"> </span><span class="p">[</span><span class="s2">"relative"</span><span class="p">,</span><span class="err"> </span><span class="s2">"${workspaceFolder}"</span><span class="p">],</span><span class="w">
    </span><span class="err">            </span><span class="nl">"pattern"</span><span class="p">:</span><span class="err"> </span><span class="p">{</span><span class="w">
    </span><span class="err">              </span><span class="nl">"regexp"</span><span class="p">:</span><span class="err"> </span><span class="s2">"^(.*):(</span><span class="se">\\</span><span class="s2">d+):(</span><span class="se">\\</span><span class="s2">d+):</span><span class="se">\\</span><span class="s2">s+(warning|error):</span><span class="se">\\</span><span class="s2">s+(.*)$"</span><span class="p">,</span><span class="w">
    </span><span class="err">              </span><span class="nl">"file"</span><span class="p">:</span><span class="err"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="err">              </span><span class="nl">"line"</span><span class="p">:</span><span class="err"> </span><span class="mi">2</span><span class="p">,</span><span class="w">
    </span><span class="err">              </span><span class="nl">"column"</span><span class="p">:</span><span class="err"> </span><span class="mi">3</span><span class="p">,</span><span class="w">
    </span><span class="err">              </span><span class="nl">"severity"</span><span class="p">:</span><span class="err"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
    </span><span class="err">              </span><span class="nl">"message"</span><span class="p">:</span><span class="err"> </span><span class="mi">5</span><span class="w">
    </span><span class="err">            </span><span class="p">}</span><span class="w">
    </span><span class="err">          </span><span class="p">},</span><span class="w">
    </span><span class="err">        </span><span class="nl">"group"</span><span class="p">:</span><span class="err"> </span><span class="p">{</span><span class="w">
    </span><span class="err">          </span><span class="nl">"kind"</span><span class="p">:</span><span class="err"> </span><span class="s2">"build"</span><span class="p">,</span><span class="w">
    </span><span class="err">          </span><span class="nl">"isDefault"</span><span class="p">:</span><span class="err"> </span><span class="kc">true</span><span class="w">
    </span><span class="err">        </span><span class="p">}</span><span class="w">
    </span><span class="err">      </span><span class="p">}</span><span class="w">
    </span><span class="err">    </span><span class="p">]</span><span class="w">
    </span><span class="err">  </span><span class="p">}</span><span class="w">
    
</span></code></pre></div></div> <h4 id="actually-building">Actually Building!</h4> <p>Now we can try to run the build task again:</p> <div class="row mt-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vscode_4_build_task_run-480.webp 480w,/assets/img/vscode_4_build_task_run-800.webp 800w,/assets/img/vscode_4_build_task_run-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vscode_4_build_task_run.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <caption class="caption"> Build task results </caption> <p>We get a terminal at the bottom reporting that the build task was run successfully.</p> <p>###Just regularly compiling</p> <p>If you wanted to skip the “build task” setup step and just jump to compiling via the terminal yourself, you can use clang like you would use gcc/g++: <code class="language-plaintext highlighter-rouge">clang++ vscode_test.cpp -o vscode_test</code></p> <h3 id="running-the-code">Running the code</h3> <p>Depending on how you specify your tasks in tasks.json you can determine where the compiled code will go. In this example it stays in the same workspace directory, so we can run <code class="language-plaintext highlighter-rouge">./vscode_test</code> directly.</p> <div class="row mt-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vscode_5_terminal_done-480.webp 480w,/assets/img/vscode_5_terminal_done-800.webp 800w,/assets/img/vscode_5_terminal_done-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vscode_5_terminal_done.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <caption class="caption"> Successful build </caption> <p>There you go!</p>]]></content><author><name></name></author><category term="debugging"/><category term="formatting"/><category term="VSCode"/><category term="C++"/><summary type="html"><![CDATA[Some common debug issues found teaching C++]]></summary></entry></feed>