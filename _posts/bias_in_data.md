# Perpetually Imperfect

Data is hot these days. Some might say it has always been hot, but it's hard to dispute the very focused hype of the last couple years around big data and data science. 

PICTURE 1
PICTURE 2
PICTURE 3

I'm excited by the potential for data-driven processes to help make the world a better place. Shipping companies are becoming more efficient, and through that, make less of an environmental impact. Farmers mix their intuition with collected data to increase crop yields. We are on the verge of personalized healthcare. These are incredible strides that are unprecedented in our history. But even with all this exciting potential and real impact, we need to be careful not to write a blank check for data science. Why? Because it's a human-driven process, and human-driven processes are fundamentally imperfect. 

You see it in the media and hear about it sometimes-– AI, data, and machine learning all enable our decisions to somehow be totally impartial, and through that, be perfect. 

"The data says so." 

Quote 2

Quote 3

But what you don't hear about are the people involved in every step of the process; the people who can either intentionally or unintentionally introduce bias. 

Think about the process like a chain, where each of the chainlinks are a step in the process from problem to solution. The whole chain is only as strong as it's weakest link. And if we're not careful, each link is weak.


PROBLEM

DATA SOURCE (& DATA PARTNERS)

MODEL

SOFTWARE IMPLEMENTATION

EVALUATION

PROCESS IMPLEMENTATION

IMPACT/FEEDBACK


## First Step

First and foremost, we need a problem to solve and someone who is interested in solving it. In mainstream discourse, we tend to talk about big diffusive problems like climate change, systematic racism and sexism, and looming nuclear war. Because these problems are so big and hard to pin down, they remain largely intractable. Maybe impossible without some genius innovation.

Naturally people have started to take these original problems and break them up into smaller more achievable tasks. Consider climate change: it's such a big problem that we likely can't solve it without some massive social impetus for change (like a law banning oil, or a boycott against gas companies). Without that impetus, you're better off with solving relatively "smaller problems", like trying to replace oil with natural gas, getting people to stop eating so much beef, encouraging solar panel research, etc.

Which leads us to the first weak link: the people who decide which subproblem to tackle and how to tackle it. Who thinks this problem is important to solve? Is it some big faceless organization? Is it some high-octane 20-something startup CEO? Your background, your access to capital, resources, education, and your primary objectives or stakes in solving the problem you set out for yourself or organization are going to be imperfect because it gives you a narrow, and ultimately incomplete view on the problem.

This doesn't stop people from moving forward with their projects. Nor should it. It's just shortsighted to think it's a perfect process, where the best ideas will always come through and manifest in some working solution. At best we see the ideas that either originate or get picked up by well-connected people with resources. It's tempting to get into a marxist analysis of the "marketplace of ideas", but that's for another post.

I'll conclude this section with a hypothetical: if a company wanted to optimize their shipping process, and they constrain their problem in such a way that they decide to only budget for a month's worth of effort from one data scientist, then they'll get 

## Second Step

Once you have a problem you want to answer, next comes the data. Sometimes there's existing data that you can use in a new way, and sometimes you have to gather your own data. Herein lies the second problem: the nature of data itself. 

To better understand this, let's think philosophically for a second. What are data, and why do we collect them? Most if not all kinds of data are instances of real-world processes. In other words, they're measurements of processes that we want to better understand. This is an idea that's easier to understand in the context of science; we collect data in high energy physics experiments so we can understand how the universe works on a fundamental level. We experiment with cancer in mice so we can understand how it might work in humans, and how we can beat it.

There are two major potential issues with data: what tools we use to collect it, and what kinds of data we collect. 

By virtue of being created by people, the tools we use to collect data are not going to be perfect. Not to worry though– statisticians and experts in these tools have devised robust techniques for taking into consideration measurement error. This leaves us with the problem of knowing what data is the best to collect. 


## Conclusion

All sorts of sciences, and really anything that makes use of data are fundamentally imperfect. But we can and should accept uncertainty in all of this. These scientists, organizations, and entrepeneurs can make the world a better place. And that's an honorable duty these people have; we just need to make sure that we can keep them accountable. 

Pobody's nerfect. 



---- this goes in a separate post

# Trying to make robust statistical analyses around data and CS experiments

Having come from a wet-lab biological world, I'd be the first to tell you that providing (and on the other side, interpreting) 
statistical tests in scientific papers can largely be unintuitive. Maybe it has something to do with probability literacy, 
or with a general difficulty in grappling with uncertainty in *empirical* evaluations of ideas, tools, and systems, but I'd like
to present a few straightforward examples of tests you can perform to have a slightly more robust analysis. 





---

iterative intersectional solution

orchestra trying to correct for nepotistic bias in how they selected new players

they used a curtain but realized that the curtain was too short so they could see feet -- didn't want to bias on gender

they could hear high heels so they put a rug in

these problems are not arrive at simply
the solutions are also not going to be simple
	we can trend towards fairness but who's to say we'll ever be done
	
at the same exact time disregarding the characteristics can be biased in their own way

accuracy fetish
	bigger social trends in AI
	
study at LSU
judges in louisiana giving harsher sentences if their favorite football teams lost the previous night
	using ML to identify and perhaps help augment the decisions, not necessarily to replace 
